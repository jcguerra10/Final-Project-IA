# -*- coding: utf-8 -*-
"""Data_Exploration_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hhF80BHf71QhSBuBGDpkEFIZcj8Rwm3k

# IMPORTS
"""

import \
    pandas as pd  # Importamos la librería pandas. Nos va a servir para leer y manipular conjuntos de datos tabulares.
import matplotlib.pyplot as plt  # Importamos pyplot de librería matplotlib. Lo vamos a utilizar para graficar.
import seaborn as sns  # Importamos la librería Seaborn. La vamos a utilizar para graficar.
import numpy as np  # Importamos la librería numpy para manipular arreglos.

from sklearn.model_selection import train_test_split  # Útil para dividir los conjuntos de datos.
from sklearn.preprocessing import MinMaxScaler  # Útil para escalar los atributos de entrada.
from sklearn import neighbors  # Permite usar KNN.
from sklearn import metrics  # Permite calcular algunas métricas de desempeño.

from sklearn.cluster import KMeans  # Clase que implementa k-means.
from sklearn.metrics import silhouette_samples  # Útil para calcular el valor de la silueta de una observación.
from sklearn.metrics import silhouette_score  # Útil para calcular el valor de la silueta de todas las observaciones.
from sklearn.metrics import calinski_harabasz_score  # Útil para calcular el valor del índice Calinski Harabasz (CH).
from sklearn.metrics import confusion_matrix  # Permite extraer la matriz de confusión.

from copy import deepcopy  # Permite hacer copias profundas.

# !pip install yellowbrick --upgrade                   #Instala y actualiza la librería yellowbrick (la versión por defecto en Google Colab está desactualizada).
from yellowbrick.cluster import \
    KElbowVisualizer  # Permite obtener la gráfica del codo para tres métricas diferentes (distorsión, silueta, CH).
from yellowbrick.cluster import SilhouetteVisualizer  # Permite obtener la gráfica de las siluetas de cada grupo.

"""# Import Dataframe"""

df = pd.read_excel("online_retail_II.xlsx")
# df = pd.read_excel('online_retail_II.xlsx')

# df.head(10)

"""# Data Exploration"""

# print('(Número de observaciones, número de atributos) = {}\n'.format(df.shape))
# print(df.dtypes, '\n')

# Modificacion de tipo de atributos

df['Invoice'] = df['Invoice'].astype("string")
df['StockCode'] = df['StockCode'].astype("string")
df['Description'] = df['Description'].astype("string")
df['Country'] = df['Country'].astype("string")

# print(df.dtypes, '\n')

"""# BASIC STATISTIC"""

# print('Estadisticas Basicas del Dataframe')
# df.describe()

# # print("Interquartile Range")
# Q1 = df.quantile(0.25)
# Q3 = df.quantile(0.75)
# IQR = Q3 - Q1
# # print(IQR)

# print('¿El conjunto de datos tiene NaN?', df.isnull().values.any(), '\n')
# print('¿Cuántos NaN tiene en total?', df.isnull().sum().sum(), '\n')

# En que  atributos hay valores nan?
# nanDictionary = {}
# for atributo in df.columns.values:
#     nanDictionary[atributo] = df[atributo].isnull().sum()
# print('¿Cómo están distribuidos los NaN?\n', nanDictionary)

df.columns[df.isnull().any()]

df['Description'].value_counts(dropna=True)

df['Customer ID'].value_counts(dropna=True)

# Cantidad de facturas con usuario nulo
df[df['Customer ID'].isnull()].Invoice.value_counts()

df_1 = df.copy()

df = df_1.copy()

df.isnull().sum()

"""#DATA CLEANING

### Eliminar valores nulos

Primero eliminamos los nulos en usuario y comprobamos nulos en producto después de la eliminación
"""

df = df.dropna(subset=["Customer ID"])

df.isnull().sum()

"""Con solo eliminar los nulos en los ususarios, ya tenemos el dataset libre de valores nulos, por lo que no se aplican más transformaciones en este aspecto."""

# print('¿El conjunto de datos tiene NaN?', df.isnull().values.any(), '\n')
# print('¿Cuántos NaN tiene en total?', df.isnull().sum().sum(), '\n')

# En que  atributos hay valores nan?
# nanDictionary = {}
# for atributo in df.columns.values:
#     nanDictionary[atributo] = df[atributo].isnull().sum()
# print('¿Cómo están distribuidos los NaN?\n', nanDictionary)

# df.shape

# df.isnull().sum()

# Clonamos el dataset original
df_clean = df.copy()

# En nuestro estudio no se esta utilizando el atributo Invoice
df_clean.drop(columns="Invoice")
# df_clean.head()

# print('Outliers')
df_clean.boxplot(column=["Price"])
# plt.show()

# print('Outliers')
df_clean.boxplot(column=["Quantity"])
# plt.show()

"""### Convertir a positivo los valores negativos de quantity"""

df_clean.Quantity = df_clean.Quantity.apply(lambda x: x * -1 if x < 0 else x)

# df_clean.info()

df_clean["Customer ID"].value_counts()

df_clean.groupby(by="Invoice").count().sort_values(by="StockCode", ascending=False)

df_clean.groupby(by="Customer ID").count().sort_values(by="StockCode", ascending=False)

df_clean[df_clean["Customer ID"] == 14911]

# df_clean.head()

df_clean["Total Price"] = df_clean["Quantity"] * df_clean["Price"]

"""### Variables derivadas / Solución de preguntas objetvos (análisis)"""

invoicexcustomer = df_clean.groupby(by="Customer ID")["Invoice"].count().reset_index()

invoicexcustomer.columns = ["Customer ID", "Customer Invoice Count"]

invoicexprice = df_clean.groupby(by="Invoice").agg({'Total Price': "sum", 'StockCode': 'count'}).reset_index()

invoicexprice.columns = ["Invoice", "Total Invoice Price", "Total Invoice Products"]

df_clean = df_clean.merge(invoicexcustomer, on="Customer ID", how="left")
df_clean = df_clean.merge(invoicexprice, on="Invoice", how="left")

# df_clean

df_clean.sort_values(by="Customer Invoice Count", ascending=False)

# df_clean.info()

# Comprobando que un cliente (se elige el de mayor cantidad de facturas) pertenece a un único país
df_clean[df_clean["Customer ID"] == 14911]["Country"].value_counts()

"""### Obtener valor de la columna de fecha"""

(df_clean['InvoiceDate'].dt.hour % 24 + 4) // 6

df_clean.sort_values(by="InvoiceDate", ascending=False)

df_clean['Day Period'] = (df_clean['InvoiceDate'].dt.hour % 24 + 4) // 6
df_clean['Day Period'].replace({
    1: 'Early Morning',
    2: 'Morning',
    3: 'Afternoon',
    4: 'Evening'}, inplace=True)

df_clean["WeekDay Type"] = df_clean["InvoiceDate"].apply(lambda x: "Weekend" if x.weekday() > 4 else "Weekday")

df_clean['Day Period'].value_counts()

df_clean["WeekDay Type"].value_counts()

# df_clean

df_clean[['Day Period', "InvoiceDate"]]

"""### Correlaciones"""

# import matplotlib.pyplot as plt

# corr_df = df_clean.corr()

# corr_df

# plt.figure(figsize=(8, 6))
# sns.heatmap(corr_df, annot=True)
# plt.show()

"""## Creación de Tabla de Clientes"""

# df_clean.head(3)

df_clean["Day Period"].value_counts()

# df_clean.columns

df_clean['Country'] = df_clean['Country'].astype("category")
df_clean['Day Period'] = df_clean['Day Period'].astype("category")
df_clean['WeekDay Type'] = df_clean['WeekDay Type'].astype("category")

customers = df_clean[
    ["Customer ID", "Invoice", "Country", "Customer Invoice Count", "Total Invoice Price", "Total Invoice Products",
     "Day Period", "WeekDay Type"]]
# nc = no country
customers_nc = df_clean[
    ["Customer ID", "Invoice", "Customer Invoice Count", "Total Invoice Price", "Total Invoice Products", "Day Period",
     "WeekDay Type"]]

# customers.info()

"""One Hot encoding de las variables obtenidas de los días"""

customers_nc_ohe = pd.get_dummies(customers_nc, columns=["Day Period", "WeekDay Type"])

# customers_nc_ohe.head()

"""Agrupar por clientes con las variables creadas"""

customers_nc_grouped = customers_nc_ohe.groupby(by=["Customer ID", "Invoice"]).mean().reset_index()

customers_nc_grouped

customers_nc_grouped["Invoice Price mean"] = customers_nc_grouped["Total Invoice Price"]
final_customers = customers_nc_grouped.groupby(by="Customer ID").agg({"Customer Invoice Count": "mean",
                                                                      "Total Invoice Price": "sum",
                                                                      "Invoice Price mean": "mean",
                                                                      "Total Invoice Products": "mean",
                                                                      "Day Period_Afternoon": "sum",
                                                                      "Day Period_Early Morning": "sum",
                                                                      "Day Period_Evening": "sum",
                                                                      "Day Period_Morning": "sum",
                                                                      "WeekDay Type_Weekday": "sum",
                                                                      "WeekDay Type_Weekend": "sum"
                                                                      }).reset_index(drop=True)

final_customers

"""### Escalar Datos"""

# final_customers.describe()

# print(final_customers.groupby('Day Period_Evening').size())

customers_scale = final_customers.copy()

rango_de_salida_de_las_variables_escaladas = (0, 1)  # Tupla con el siguiente formato: (mínimo deseado, máximo deseado).
scaler = MinMaxScaler(
    feature_range=rango_de_salida_de_las_variables_escaladas)  # Instanciamos el objeto para escalar los datos.

customers_scale[:] = scaler.fit_transform(customers_scale[:])  # Ajustamos y transformamos los datos.

# print('-----------------------------------------------------------------------')
# print('Datos de entrada del conjunto de entrenamiento ANTES del escalado')
# print('-----------------------------------------------------------------------')
# print(final_customers.describe(), '\n')
#
# print('-----------------------------------------------------------------------')
# print('Datos de entrada del conjunto de entrenamiento DESPUÉS del escalado')
# print('-----------------------------------------------------------------------')
# print(customers_scale.describe())

"""# Modelos

## Clustering K-means
"""

# -------------------------------------------------------------------------------
# -------------------------------------------------------------------------------
# I. MÉTODO DEL CODO
# -------------------------------------------------------------------------------
# -------------------------------------------------------------------------------

kmin = 1  # Límite inferior para explorar el número de grupos.
kmax = 10  # Límite superior para explorar el número de grupos.
init = 'k-means++'  # Se define el método de inicialización. Otra opción válida es 'random'.
n_init = 10  # Número de inicializaciones aleatorias. Al final scikit learn escoge aquel con la menor inercia
# (i.e.: suma de cuadrados de distancias de cada punto a su centroide respectivo dentro de cada grupo, para todos los puntos).
# https://scikit-learn.org/stable/modules/clustering.html
max_iter = 300  # Número MÁXIMO de iteraciones para una sola ejecución.
random_seed = 47  # Semilla aleatoria. Permite obtener los mismos resultados en cada ejecución.

# num_observaciones = float(df_x.shape[0]) #Número de observaciones o puntos que tendría el conjunto de datos.

# -------------------------------------------------------------------------------
# Opción 1: usando scikit-learn y matplotlib.
# -------------------------------------------------------------------------------

k_list = list(np.arange(kmin, kmax + 1, 1))  # Lista con los valores de k que se van a explorar [kmin,kmax].
inertia_list = []  # Lista vacía. Va a almacenar los valores de inercia para cada valor de k.
# La inercia = sumatoria( ( x_i-centroide(x_i) )^2 ).
# La inercia = sumatoria de los cuadrados de las distancias de las observaciones al centroide más cercano.
silhouette_score_list = []  # Lista vacía. Va a almacenar los valores de silueta promedio para todas las observaciones para cada valor de k.
calinski_harabasz_score_list = []  # Lista vacía. Va a almacenar los valores del índice Calinski Harabasz para cada valor de k.

for k in k_list:  # Explore los valores de k en la lista.
    # Se instancia el objeto para utilizar el agrupamiento con k-means.
    # Para ver todas los opciones del constructor, consulte: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
    # Nota: el algoritmo de k-means disponible en scikit-learn funciona únicamente con la distancia euclidiana.
    # Si usted requiere aplicar k-means con otras métricas de distancia, puede consultar la librería PyClustering: https://github.com/annoviko/pyclustering
    model1 = KMeans(n_clusters=k,  # Se define el número de grupos.
                    init=init,  # Se define el método de inicialización.
                    n_init=n_init,
                    # Número de inicializaciones aleatorias. Al final se escoge aquel con la menor inercia.
                    max_iter=max_iter,  # Número MÁXIMO de iteraciones para una sola ejecución.
                    random_state=random_seed)

    # Se hace el ajuste (i.e.: encontramos los centroides).
    model1.fit(customers_scale)

    # Agregamos a la lista el valor de inercia que se obtiene para el número actual de grupos.
    inertia_list.append(model1.inertia_)

    if k == 1:  # En caso que se esté evaluando el caso cuando el número de grupos es 1...
        silhouette_score_list.append(np.nan)  # Esto sirve como recordatorio que no se puede calcular este valor.
        calinski_harabasz_score_list.append(
            np.nan)  # Esto sirve como recordatorio que no se puede calcular este índice.
    else:
        # Agregamos a la lista el valor de la silueta promedio para todas las observaciones.
        silhouette_score_list.append(silhouette_score(customers_scale.values, model1.labels_))
        calinski_harabasz_score_list.append(calinski_harabasz_score(customers_scale.values,
                                                                    model1.labels_))  # model1.labels_ es un arreglo numpy de orden 1 (i.e. no es vector fila, no es vector columna...)

# -------------------------------------------------------------------------------
# K-means
# -------------------------------------------------------------------------------
k = 4  # Número de grupos que se escogió después del análisis previo.

# Ahora se instancia el objeto para utilizar el agrupamiento con k-means.
# Para ver todas los opciones del constructor, consulte: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
# Nota: el algoritmo de k-means disponible en scikit-learn funciona únicamente con la distancia euclidiana.
# Si requiere aplicar k-means con otras métricas de distancia, puede consultar la librería PyClustering: https://github.com/annoviko/pyclustering
kmeans = KMeans(n_clusters=k,  # Se define el número de grupos.
                init=init,  # Se define el método de inicialización. Otra opción es 'random'
                n_init=n_init,  # Número de inicializaciones aleatorias.
                max_iter=max_iter,  # Número MÁXIMO de iteraciones para una sola ejecución.
                random_state=random_seed)

# Hagamos el ajuste (i.e.: encontremos los centroides).
kmeans.fit(customers_scale)

# Revisemos los centroides de cada grupo.
centroides = kmeans.cluster_centers_
# print('Centroides:', centroides)

plt.figure(figsize=(20, 20))
plt.scatter(customers_scale.iloc[:, 0], customers_scale.iloc[:, 1], c=kmeans.labels_.reshape((-1, 1)), s=200,
            cmap='viridis')  # Observaciones.
plt.scatter(centroides[:, 0], centroides[:, 1], marker='o', color='white', edgecolor='k', s=300,
            alpha=0.9)  # Centroides.
plt.xlabel(customers_scale.columns[0] + ' normalizado.')
plt.ylabel(customers_scale.columns[1] + ' normalizado.')

# Esto permite identificar cada centroide con su índice asociado.
for i, c in enumerate(centroides):
    plt.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')

plt.title('Resultado del agrupamiento')


def result_chart():
    return plt


# customers_scale.columns

"""Interpretar cada cluster de acuerdo a los valores de los centroides"""

customers_scale

"""## Sistema de recomendación - Algoritmo A Priori"""

from mlxtend.frequent_patterns import apriori, association_rules

df_clean.Description = df_clean.Description.str.strip()

#Apriori
data = deepcopy(df)

data['Description'] = data['Description'].str.strip()

# Dropping the rows without any invoice number
data['Invoice'] = data['Invoice'].astype('str')

# Dropping all transactions which were done on credit
data = data[~data['Invoice'].str.contains('C')]

basket_France = (data[data['Country'] == "France"]
                 .groupby(['Invoice', 'Description'])['Quantity']
                 .sum().unstack().reset_index().fillna(0)
                 .set_index('Invoice'))

# Transactions done in the United Kingdom
basket_UK = (data[data['Country'] == "United Kingdom"]
             .groupby(['Invoice', 'Description'])['Quantity']
             .sum().unstack().reset_index().fillna(0)
             .set_index('Invoice'))

# Transactions done in Portugal
basket_Por = (data[data['Country'] == "Portugal"]
              .groupby(['Invoice', 'Description'])['Quantity']
              .sum().unstack().reset_index().fillna(0)
              .set_index('Invoice'))

basket_Sweden = (data[data['Country'] == "Sweden"]
                 .groupby(['Invoice', 'Description'])['Quantity']
                 .sum().unstack().reset_index().fillna(0)
                 .set_index('Invoice'))


def hot_encode(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1


# Encoding the datasets
basket_encoded = basket_France.applymap(hot_encode)
basket_France = basket_encoded

basket_encoded = basket_UK.applymap(hot_encode)
basket_UK = basket_encoded

basket_encoded = basket_Por.applymap(hot_encode)
basket_Por = basket_encoded

basket_encoded = basket_Sweden.applymap(hot_encode)
basket_Sweden = basket_encoded

# Building the model
frq_items_fr = apriori(basket_France, min_support=0.05, use_colnames=True, low_memory=True)
frq_items_uk = apriori(basket_UK, min_support=0.05, use_colnames=True, low_memory=True)
frq_items_pr = apriori(basket_Por, min_support=0.05, use_colnames=True, low_memory=True)
frq_items_sw = apriori(basket_Sweden, min_support=0.05, use_colnames=True, low_memory=True)


def apriori_alg(x):
    frq_items: None
    if x == 'France':
        frq_items = frq_items_fr
    elif x == 'United Kingdom':
        frq_items = frq_items_uk
    elif x == 'Portugal':
        frq_items = frq_items_pr
    elif x == 'Sweden':
        frq_items = frq_items_sw
    rules = association_rules(frq_items, metric="lift", min_threshold=1)
    rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])
    return rules
